{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled22.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOtlMHyZ5yy64Aiw9netsvi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/teamgaon/KIST/blob/main/220515_KIST_CNN2RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8fUQIVY6WRE",
        "outputId": "e7d9e11e-2cd6-40d5-8e32-15d354405495"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ttach\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: ttach\n",
            "Successfully installed ttach-0.0.3\n",
            "Collecting albumentations==0.4.6\n",
            "  Downloading albumentations-0.4.6.tar.gz (117 kB)\n",
            "\u001b[K     |████████████████████████████████| 117 kB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.21.6)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Collecting imgaug>=0.4.0\n",
            "  Downloading imgaug-0.4.0-py2.py3-none-any.whl (948 kB)\n",
            "\u001b[K     |████████████████████████████████| 948 kB 40.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.1.post1)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.8)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.2.0)\n",
            "Building wheels for collected packages: albumentations\n",
            "  Building wheel for albumentations (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for albumentations: filename=albumentations-0.4.6-py3-none-any.whl size=65174 sha256=fc7c1d7c169c74ed2ebd0f705aa6f0ebe331a13605d24488ec75781de7c47a42\n",
            "  Stored in directory: /root/.cache/pip/wheels/cf/34/0f/cb2a5f93561a181a4bcc84847ad6aaceea8b5a3127469616cc\n",
            "Successfully built albumentations\n",
            "Installing collected packages: imgaug, albumentations\n",
            "  Attempting uninstall: imgaug\n",
            "    Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Attempting uninstall: albumentations\n",
            "    Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.4.6 imgaug-0.4.0\n"
          ]
        }
      ],
      "source": [
        "!pip install ttach\n",
        "!pip install albumentations==0.4.6"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi\n",
        "# k80 -> T4 -> P100"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJUriUI66aB9",
        "outputId": "856bb9fb-de4c-4550-a867-5bb92d9b4a1f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun May 15 06:44:35 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P8    11W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import albumentations\n",
        "import albumentations.pytorch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset\n",
        "from sklearn.metrics import f1_score\n",
        "import ttach as tta\n",
        "import albumentations as A\n",
        "from sklearn.model_selection import KFold, StratifiedKFold\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "from datetime import datetime, timedelta\n",
        "import random\n",
        "import torchvision.transforms as transforms\n",
        "import gc\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "AFpTjc-M6c2r"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3GA6h9ts6eHz",
        "outputId": "745c8c28-c3f0-4a1c-eaa7-e292ccb20419"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_cases = glob('/content/drive/MyDrive/KIST/open/train/*')\n",
        "train_cases.sort()"
      ],
      "metadata": {
        "id": "Y0JCUMSC7Bvt"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = glob(train_cases[0]+'/image/*')\n",
        "images.sort()\n",
        "label = pd.read_csv(train_cases[0]+'/label.csv')\n",
        "for i in range(len(images)):\n",
        "  image = cv2.imread(images[i])\n",
        "  print(images[i][-13:])\n",
        "  print('leaf_weight : ' + str(label['leaf_weight'].loc[i]))\n",
        "  plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "ZwkEK-C97ER7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # 분석에 사용할 feature 선택\n",
        "# csv_features = ['내부온도관측치', '외부온도관측치', '내부습도관측치', '외부습도관측치', 'CO2관측치', 'EC관측치',\n",
        "#        '최근분무량', '화이트 LED동작강도', '레드 LED동작강도', '블루 LED동작강도', '냉방온도', '냉방부하',\n",
        "#        '난방온도', '난방부하', '총추정광량', '백색광추정광량', '적색광추정광량']\n",
        "\n",
        "# csv_files = sorted(glob('/content/drive/MyDrive/KIST/open/train/*/meta/*'))\n",
        "\n",
        "# temp_csv = pd.read_csv(csv_files[0])[csv_features]\n",
        "# max_arr, min_arr = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
        "\n",
        "# # feature 별 최대값, 최솟값 계산\n",
        "# for csv in tqdm(csv_files[1:]):\n",
        "#     temp_csv = pd.read_csv(csv)[csv_features]\n",
        "#     temp_csv = temp_csv.fillna(0)\n",
        "#     if len(temp_csv) == 0:\n",
        "#         continue\n",
        "#     temp_csv = temp_csv.astype(float)\n",
        "#     temp_max, temp_min = temp_csv.max().to_numpy(), temp_csv.min().to_numpy()\n",
        "#     max_arr = np.max([max_arr,temp_max], axis=0)\n",
        "#     min_arr = np.min([min_arr,temp_min], axis=0)\n",
        "\n",
        "# # feature 별 최대값, 최솟값 dictionary 생성\n",
        "# csv_feature_dict = {csv_features[i]:[min_arr[i], max_arr[i]] for i in range(len(csv_features))}\n",
        "# csv_feature_dict"
      ],
      "metadata": {
        "id": "W88ZQexz7ali"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_feature_dict = {'CO2관측치': [0.0, 1757.0],\n",
        " 'EC관측치': [-0.00080744, 43.59114074707031],\n",
        " '난방부하': [0.0, 68.50000381469727],\n",
        " '난방온도': [0.0, 26.0],\n",
        " '내부습도관측치': [0.0, 110.6199913],\n",
        " '내부온도관측치': [0.0, 40.59999847412109],\n",
        " '냉방부하': [0.0, 403.5995],\n",
        " '냉방온도': [0.0, 28.0],\n",
        " '레드 LED동작강도': [0.0, 201.0],\n",
        " '백색광추정광량': [0.0, 309.41],\n",
        " '블루 LED동작강도': [0.0, 201.0],\n",
        " '외부습도관측치': [0.0, 201.0],\n",
        " '외부온도관측치': [0.0, 89.4000015258789],\n",
        " '적색광추정광량': [0.0, 165.48],\n",
        " '총추정광량': [0.0, 631.54],\n",
        " '최근분무량': [0.0, 642997.3999999999],\n",
        " '화이트 LED동작강도': [0.0, 201.0]}"
      ],
      "metadata": {
        "id": "uaNdSMu6HOra"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = pd.DataFrame({'img':[0],'csv':[0],'weight':[0]})\n",
        "for case in tqdm(train_cases):\n",
        "  img_train = sorted(glob(case + '/image/*'))\n",
        "  csv_train = sorted(glob(case + '/meta/*'))\n",
        "  weights = pd.read_csv(case + '/label.csv')['leaf_weight']\n",
        "  temp = pd.DataFrame({'img':img_train,'csv':csv_train,'weight':weights})\n",
        "  train=pd.concat([train,temp], axis=0)\n",
        "train = train[1:].reset_index(drop=True)\n",
        "train"
      ],
      "metadata": {
        "id": "JUEJSAL0dLZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img_files = sorted(glob('/content/drive/MyDrive/KIST/open/test/image/*'))\n",
        "metas = sorted(glob('/content/drive/MyDrive/KIST/open/test/meta/*'))\n",
        "test = pd.DataFrame({'img':img_files, 'csv':metas})\n",
        "test"
      ],
      "metadata": {
        "id": "HsMRn-D9hcUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, files, labels=None, mode='train', transforms=None):\n",
        "        self.mode = mode\n",
        "        self.files = files\n",
        "        self.csv_feature_dict = csv_feature_dict\n",
        "        self.csv_feature_check = [0]*len(self.files)\n",
        "        self.csv_features = [None]*len(self.files)\n",
        "        self.max_len = 24 * 6\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "    \n",
        "    def __getitem__(self, i):\n",
        "        # file = self.files[i]\n",
        "        # file_name = file.split('/')[-1]\n",
        "        image_path = self.files['img'][i]\n",
        "        csv_path = self.files['csv'][i]\n",
        "        \n",
        "        # csv\n",
        "        if self.csv_feature_check[i] == 0:\n",
        "            df = pd.read_csv(csv_path)\n",
        "            df = df[self.csv_feature_dict.keys()]\n",
        "            df = df.fillna(0)\n",
        "            # MinMax scaling\n",
        "            for col in df.columns:\n",
        "                df[col] = df[col].astype(float) - self.csv_feature_dict[col][0]\n",
        "                df[col] = df[col] / (self.csv_feature_dict[col][1]-self.csv_feature_dict[col][0])\n",
        "            # zero padding\n",
        "            pad = np.zeros((self.max_len, len(df.columns)))\n",
        "            length = min(self.max_len, len(df))\n",
        "            pad[-length:] = df.to_numpy()[-length:]\n",
        "            # transpose to sequential data\n",
        "            csv_feature = pad.T\n",
        "            self.csv_features[i] = csv_feature\n",
        "            self.csv_feature_check[i] = 1\n",
        "        else:\n",
        "            csv_feature = self.csv_features[i]\n",
        "        \n",
        "        # image\n",
        "        img = cv2.imread(image_path)\n",
        "        if self.transforms is not None:\n",
        "          img = self.transforms(image=img)['image']\n",
        "        \n",
        "        if self.mode == 'train':\n",
        "            weight = self.files['weight'][i]\n",
        "            # json_path = f'{file}/{file_name}.json'\n",
        "            # with open(json_path, 'r') as f:\n",
        "            #     json_file = json.load(f)\n",
        "            \n",
        "            # crop = json_file['annotations']['crop']\n",
        "            # disease = json_file['annotations']['disease']\n",
        "            # risk = json_file['annotations']['risk']\n",
        "            # label = f'{crop}_{disease}_{risk}'\n",
        "            \n",
        "            return {\n",
        "                'img' : img,\n",
        "                'csv_feature' : torch.tensor(csv_feature, dtype=torch.float32),\n",
        "                # 타겟 타입 float로 바꾸기\n",
        "                'target' : torch.tensor(weight, dtype=torch.float32)\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                'img' : img,\n",
        "                'csv_feature' : torch.tensor(csv_feature, dtype=torch.float32)\n",
        "            }"
      ],
      "metadata": {
        "id": "9NFUVe92XoUx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\")\n",
        "batch_size = 4\n",
        "# class_n = len(label_encoder)\n",
        "learning_rate = 1e-4\n",
        "embedding_dim = 600\n",
        "height = 600\n",
        "width = 600\n",
        "num_features = len(csv_feature_dict)\n",
        "max_len = 24*6\n",
        "dropout_rate = 0.1\n",
        "epochs = 20\n",
        "k_folds = 5\n",
        "vision_pretrain = True\n",
        "save_path = '/content/drive/MyDrive/220515/'"
      ],
      "metadata": {
        "id": "WfZ-_ChRg4yN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_features"
      ],
      "metadata": {
        "id": "dL7yCmKat1Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train = sorted(glob('/content/train/*'))\n",
        "# test = sorted(glob('/content/test/*'))\n",
        "\n",
        "# labelsss = pd.read_csv('/content/drive/MyDrive/LG/train.csv')['label']"
      ],
      "metadata": {
        "id": "QlSHRQGwhGzs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_transforms = A.Compose([\n",
        "#         A.Resize(height=height, width=width, interpolation=cv2.INTER_AREA),\n",
        "#         A.HorizontalFlip(p=0.5),\n",
        "#         A.VerticalFlip(p=0.5),\n",
        "#         A.Rotate(25, interpolation=cv2.INTER_AREA),\n",
        "#         A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "#         ToTensorV2(),\n",
        "# ])\n",
        "\n",
        "train_transforms = A.Compose([\n",
        "        A.Resize(height=height, width=width, interpolation=cv2.INTER_AREA),\n",
        "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "])"
      ],
      "metadata": {
        "id": "y0_FG6rKh5_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_transforms = A.Compose([\n",
        "        A.Resize(height=height, width=width, interpolation=cv2.INTER_AREA),\n",
        "        A.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
        "        ToTensorV2(),\n",
        "    ])"
      ],
      "metadata": {
        "id": "6X9bIVvch7Qv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(train, transforms = train_transforms)\n",
        "test_dataset = CustomDataset(test, mode = 'test', transforms = test_transforms)\n",
        "\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, num_workers=2, shuffle=False)"
      ],
      "metadata": {
        "id": "rydGv20biMlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "next(iter(train_dataset))"
      ],
      "metadata": {
        "id": "ovEU274oibhz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_Encoder(nn.Module):\n",
        "    def __init__(self, rate=0.1):\n",
        "        super(CNN_Encoder, self).__init__()\n",
        "        self.model = models.convnext_large(pretrained=True)\n",
        "        # self.model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        output = self.model(inputs)\n",
        "        return output"
      ],
      "metadata": {
        "id": "XZakLnXHi4Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RNN_Decoder(nn.Module):\n",
        "    def __init__(self, max_len, embedding_dim, num_features, rate):\n",
        "        super(RNN_Decoder, self).__init__()\n",
        "        self.lstm = nn.LSTM(max_len, embedding_dim)\n",
        "        self.rnn_fc = nn.Linear(num_features*embedding_dim, 100)\n",
        "        self.final_layer = nn.Linear(100 + 1000, 1) # resnet out_dim + lstm out_dim\n",
        "        self.dropout = nn.Dropout(rate)\n",
        "\n",
        "    def forward(self, enc_out, dec_inp):\n",
        "        hidden, _ = self.lstm(dec_inp)\n",
        "        hidden = hidden.view(hidden.size(0), -1)\n",
        "        hidden = self.rnn_fc(hidden)\n",
        "        concat = torch.cat([enc_out, hidden], dim=1) # enc_out + hidden \n",
        "        fc_input = concat\n",
        "        output = self.dropout((self.final_layer(fc_input)))\n",
        "        return output"
      ],
      "metadata": {
        "id": "roJFEiI5i6ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN2RNN(nn.Module):\n",
        "    def __init__(self, max_len, embedding_dim, num_features, rate):\n",
        "        super(CNN2RNN, self).__init__()\n",
        "        self.cnn = CNN_Encoder(rate)\n",
        "        self.rnn = RNN_Decoder(max_len, embedding_dim, num_features, rate)\n",
        "        \n",
        "    def forward(self, img, seq):\n",
        "        cnn_output = self.cnn(img)\n",
        "        output = self.rnn(cnn_output, seq)\n",
        "        \n",
        "        return output"
      ],
      "metadata": {
        "id": "gZZdPliwjEF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = models.convnext_large(pretrained=True)\n",
        "# model = model.to(device)\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'deeplabv3_resnet50', pretrained=True)"
      ],
      "metadata": {
        "id": "XyaMbuCwMJZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습"
      ],
      "metadata": {
        "id": "aQ620FNKMK5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
        "                                        lr_lambda=lambda epoch: 0.95 ** epoch,\n",
        "                                        last_epoch=-1,\n",
        "                                        verbose=False)\n",
        "\n",
        "criterion = nn.L1Loss()"
      ],
      "metadata": {
        "id": "ZOfojonSMLlI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_absolute_error"
      ],
      "metadata": {
        "id": "BHINaeBGjv31"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy_function(real, pred):    \n",
        "    real = real.cpu().detach().numpy()\n",
        "    pred = pred.cpu().detach().numpy()\n",
        "    score = mean_absolute_error(real, pred)\n",
        "    return score\n",
        "\n",
        "def train_step(batch_item, training):\n",
        "    img = batch_item['img'].to(device)\n",
        "    csv_feature = batch_item['csv_feature'].to(device)\n",
        "    label = batch_item['target'].to(device)\n",
        "    new_shape = (len(label), 1)\n",
        "    label = label.view(new_shape)\n",
        "    if training is True:\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        with torch.cuda.amp.autocast():\n",
        "            output = model(img, csv_feature)\n",
        "            loss = criterion(output, label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        score = accuracy_function(label, output)\n",
        "        return loss, score\n",
        "    else:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            output = model(img, csv_feature)\n",
        "            loss = criterion(output, label)\n",
        "        score = accuracy_function(label, output)\n",
        "        return loss, score"
      ],
      "metadata": {
        "id": "avDFBpqSjJBk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# loss_plot, val_loss_plot = [], []\n",
        "# metric_plot, val_metric_plot = [], []\n",
        "\n",
        "# Define the K-fold Cross Validator\n",
        "kfold = KFold(n_splits=k_folds, shuffle=True)\n",
        "\n",
        "# Start print\n",
        "print('--------------------------------')\n",
        "\n",
        "# K-fold Cross Validation model evaluation\n",
        "for fold, (train_ids, test_ids) in enumerate(kfold.split(train_dataset)):\n",
        "  my_file = Path(save_path+'best_model'+str(fold)+'.pt')\n",
        "\n",
        "  gc.collect()\n",
        "  loss_plot, val_loss_plot = [], []\n",
        "  metric_plot, val_metric_plot = [], []\n",
        "  \n",
        "  # Print\n",
        "  print('')\n",
        "  print(f'FOLD {fold}')\n",
        "  print('--------------------------------')\n",
        "\n",
        "  if my_file.is_file():\n",
        "    continue\n",
        "  \n",
        "  # Sample elements randomly from a given list of ids, no replacement.\n",
        "  train_subsampler = torch.utils.data.SubsetRandomSampler(train_ids)\n",
        "  test_subsampler = torch.utils.data.SubsetRandomSampler(test_ids)\n",
        "  \n",
        "  # Define data loaders for training and testing data in this fold\n",
        "  train_dataloader = torch.utils.data.DataLoader(\n",
        "                    train_dataset, \n",
        "                    batch_size=batch_size, sampler=train_subsampler, num_workers=2)\n",
        "  val_dataloader = torch.utils.data.DataLoader(\n",
        "                    train_dataset,\n",
        "                    batch_size=batch_size, sampler=test_subsampler, num_workers=2)\n",
        "  \n",
        "  model = CNN2RNN(max_len=max_len, embedding_dim=embedding_dim, num_features=num_features, rate=dropout_rate)\n",
        "  # model.load_state_dict(torch.load(save_path+'pretrained_model.pt', map_location=device), strict=False)\n",
        "  model = model.to(device)\n",
        "\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer,\n",
        "                                          lr_lambda=lambda epoch: 0.95 ** epoch,\n",
        "                                          last_epoch=-1,\n",
        "                                          verbose=False)\n",
        "\n",
        "  criterion = nn.L1Loss()\n",
        "  \n",
        "  for epoch in range(epochs):\n",
        "      gc.collect()\n",
        "      total_loss, total_val_loss = 0, 0\n",
        "      total_acc, total_val_acc = 0, 0\n",
        "      \n",
        "      tqdm_dataset = tqdm(enumerate(train_dataloader))\n",
        "      training = True\n",
        "      for batch, batch_item in tqdm_dataset:\n",
        "          batch_loss, batch_acc = train_step(batch_item, training)\n",
        "          total_loss += batch_loss\n",
        "          total_acc += batch_acc\n",
        "          \n",
        "          tqdm_dataset.set_postfix({\n",
        "              'Epoch': epoch + 1,\n",
        "              'Loss': '{:06f}'.format(batch_loss.item()),\n",
        "              'Mean Loss' : '{:06f}'.format(total_loss/(batch+1)),\n",
        "              'Mean MAE' : '{:06f}'.format(total_acc/(batch+1))\n",
        "          })\n",
        "      loss_plot.append(total_loss/(batch+1))\n",
        "      metric_plot.append(total_acc/(batch+1))\n",
        "      \n",
        "      tqdm_dataset = tqdm(enumerate(val_dataloader))\n",
        "      training = False\n",
        "      for batch, batch_item in tqdm_dataset:\n",
        "          batch_loss, batch_acc = train_step(batch_item, training)\n",
        "          total_val_loss += batch_loss\n",
        "          total_val_acc += batch_acc\n",
        "          \n",
        "          tqdm_dataset.set_postfix({\n",
        "              'Epoch': epoch + 1,\n",
        "              'Val Loss': '{:06f}'.format(batch_loss.item()),\n",
        "              'Mean Val Loss' : '{:06f}'.format(total_val_loss/(batch+1)),\n",
        "              'Mean Val MAE' : '{:06f}'.format(total_val_acc/(batch+1))\n",
        "          })\n",
        "      val_loss_plot.append(total_val_loss/(batch+1))\n",
        "      val_metric_plot.append(total_val_acc/(batch+1))\n",
        "      scheduler.step()\n",
        "\n",
        "      if np.min(val_metric_plot) == val_metric_plot[-1]:\n",
        "          torch.save(model.state_dict(), save_path+'best_model'+str(fold)+'.pt')\n",
        "          print('best')"
      ],
      "metadata": {
        "id": "jIu2H1L9kNR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 4"
      ],
      "metadata": {
        "id": "xuDkE5CxlQrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(dataset, model0, model1, model2, model3, model4):\n",
        "    model0.eval()\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "    model3.eval()\n",
        "    model4.eval()\n",
        "    tqdm_dataset = tqdm(enumerate(dataset))\n",
        "    results = []\n",
        "    for batch, batch_item in tqdm_dataset:\n",
        "      img = batch_item['img'].to(device)\n",
        "      seq = batch_item['csv_feature'].to(device)\n",
        "      with torch.no_grad():\n",
        "          output = 0.2*model0(img, seq) + 0.2*model1(img, seq) + 0.2*model2(img, seq) + 0.2*model3(img, seq) + 0.2*model4(img, seq)\n",
        "      # output = torch.tensor(torch.argmax(output, dim=1), dtype=torch.int32).cpu().numpy()\n",
        "      results.extend(output.cpu().numpy())\n",
        "    return results\n",
        "\n",
        "model0 = CNN2RNN(max_len=max_len, embedding_dim=embedding_dim, num_features=num_features, rate=dropout_rate)\n",
        "model0.load_state_dict(torch.load(save_path+'best_model'+str(0)+'.pt', map_location=device))\n",
        "model0.to(device)\n",
        "\n",
        "model1 = CNN2RNN(max_len=max_len, embedding_dim=embedding_dim, num_features=num_features, rate=dropout_rate)\n",
        "model1.load_state_dict(torch.load(save_path+'best_model'+str(1)+'.pt', map_location=device))\n",
        "model1.to(device)\n",
        "\n",
        "model2 = CNN2RNN(max_len=max_len, embedding_dim=embedding_dim, num_features=num_features, rate=dropout_rate)\n",
        "model2.load_state_dict(torch.load(save_path+'best_model'+str(2)+'.pt', map_location=device))\n",
        "model2.to(device)\n",
        "\n",
        "model3 = CNN2RNN(max_len=max_len, embedding_dim=embedding_dim, num_features=num_features, rate=dropout_rate)\n",
        "model3.load_state_dict(torch.load(save_path+'best_model'+str(3)+'.pt', map_location=device))\n",
        "model3.to(device)\n",
        "\n",
        "model4 = CNN2RNN(max_len=max_len, embedding_dim=embedding_dim, num_features=num_features, rate=dropout_rate)\n",
        "model4.load_state_dict(torch.load(save_path+'best_model'+str(4)+'.pt', map_location=device))\n",
        "model4.to(device)\n",
        "\n",
        "preds = predict(test_dataloader, model0, model1, model2, model3, model4)"
      ],
      "metadata": {
        "id": "zSUar4tau4-5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(preds)"
      ],
      "metadata": {
        "id": "jgv9SuKb1XKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission = pd.read_csv('/content/drive/MyDrive/KIST/open/sample_submission.csv')"
      ],
      "metadata": {
        "id": "cFnwcOni1bHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission['leaf_weight'] = np.array(preds).reshape(1,-1)[0]\n",
        "submission"
      ],
      "metadata": {
        "id": "qTCT8knY1ctR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.loc[submission['leaf_weight']<0, 'leaf_weight'] = 0"
      ],
      "metadata": {
        "id": "x-sLt6cA4bfB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = glob('/content/drive/MyDrive/KIST/open/test/image/*')\n",
        "images.sort()\n",
        "for i in range(10):\n",
        "  image = cv2.imread(images[i])\n",
        "  print(images[i][-13:])\n",
        "  print('leaf_weight : ' + str(submission['leaf_weight'].loc[i]))\n",
        "  plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "EYdhPUAy2tkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "submission.to_csv('/content/drive/MyDrive/KIST/submission.csv', index=False)"
      ],
      "metadata": {
        "id": "magKURk-4KPW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}